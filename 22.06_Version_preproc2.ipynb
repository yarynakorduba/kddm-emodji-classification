{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patri\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "#from wordclou import WordCloud\n",
    "import re\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "import torch\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(\"C:/Users/patri/Desktop/Informatik/Knowledge Discovery and Data Mining 1/emoji/emoji_train.pkl\")      # Shape: (42627, 4)\n",
    "test = pd.read_pickle(\"C:/Users/patri/Desktop/Informatik/Knowledge Discovery and Data Mining 1/emoji/emoji_test.pkl\")        # (10657, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last column in df_train is null - we remove it\n",
    "# we drop also emoji column - we don't need it\n",
    "df_train = df_train.drop(columns = [\"predicted_class\", \"emoji\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_match(contraction):\n",
    "    contraction_mapping = {\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"hasn’t\": \"has not\",\n",
    "        \"don’t\": \"do not\",\n",
    "        \"haven’t\": \"have not\",\n",
    "        \"doesn’t\": \"does not\",\n",
    "        \"couldn’t\": \"could not\",\n",
    "        \"wasn’t\": \"was not\",\n",
    "        \"hadn’t\": \"had not\",\n",
    "        \"didn’t\": \"did not\",\n",
    "        \"weren’t\": \"were not\",\n",
    "        \"wouldn’t\": \"would not\",\n",
    "        \"shouldn’t\": \"should not\",\n",
    "        \"mustn’t\": \"must not\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"who’s\": \"who is\",\n",
    "        \"who’d\": \"who would\",\n",
    "        \"who’ll\": \"who will\",\n",
    "        \"what’s\": \"what is\",\n",
    "        \"what’ll\": \"what will\",\n",
    "        \"how’s\": \"how is\",\n",
    "        \"where’s\": \"where is\",\n",
    "        \"when’s\": \"when is\",\n",
    "        \"here’s\": \"here is\",\n",
    "        \"there’s\": \"there is\",\n",
    "        \"there’d\": \"there would\",\n",
    "        \"there’ll\": \"there will\",\n",
    "        \"that’s\": \"that is\"\n",
    "    }\n",
    "    match = contraction.group(0)\n",
    "    first_char = match[0]\n",
    "    expanded_contraction = contraction_mapping.get(match) \\\n",
    "        if contraction_mapping.get(match) \\\n",
    "        else contraction_mapping.get(match.lower())\n",
    "    expanded_contraction = first_char + expanded_contraction[1:]\n",
    "    return expanded_contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(text, stem_flag = False):\n",
    "    url_pattern = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "    emojis_pattern = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "    hash_pattern = re.compile(r'#\\w*')\n",
    "    single_letter_pattern = re.compile(r'(?<![\\w\\-])\\w(?![\\w\\-])')\n",
    "    blank_spaces_pattern = re.compile(r'\\s{2,}|\\t')\n",
    "    reserved_pattern = re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n",
    "    mention_pattern = re.compile(r'@\\w*')\n",
    "    CONTRACTION_MAP = {\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"hasn’t\": \"has not\",\n",
    "        \"don’t\": \"do not\",\n",
    "        \"haven’t\": \"have not\",\n",
    "        \"doesn’t\": \"does not\",\n",
    "        \"couldn’t\": \"could not\",\n",
    "        \"wasn’t\": \"was not\",\n",
    "        \"hadn’t\": \"had not\",\n",
    "        \"didn’t\": \"did not\",\n",
    "        \"weren’t\": \"were not\",\n",
    "        \"wouldn’t\": \"would not\",\n",
    "        \"shouldn’t\": \"should not\",\n",
    "        \"mustn’t\": \"must not\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"who’s\": \"who is\",\n",
    "        \"who’d\": \"who would\",\n",
    "        \"who’ll\": \"who will\",\n",
    "        \"what’s\": \"what is\",\n",
    "        \"what’ll\": \"what will\",\n",
    "        \"how’s\": \"how is\",\n",
    "        \"where’s\": \"where is\",\n",
    "        \"when’s\": \"when is\",\n",
    "        \"here’s\": \"here is\",\n",
    "        \"there’s\": \"there is\",\n",
    "        \"there’d\": \"there would\",\n",
    "        \"there’ll\": \"there will\",\n",
    "        \"that’s\": \"that is\"\n",
    "    }\n",
    "    constraction_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())),\n",
    "                                        flags=re.IGNORECASE | re.DOTALL)\n",
    "    Whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "    urls = re.sub(pattern=url_pattern, repl='', string=text)\n",
    "    mentions = re.sub(pattern=mention_pattern, repl='', string=urls)\n",
    "    hashtag = re.sub(pattern=hash_pattern, repl='', string=mentions)\n",
    "    reserved = re.sub(pattern=reserved_pattern, repl='', string=hashtag)\n",
    "    reserved = Whitespace.sub(\" \", reserved)\n",
    "    reserved = constraction_pattern.sub(expand_match, reserved)\n",
    "    punct = \"[{}]+\".format(string.punctuation)\n",
    "    punctuation = re.sub(punct, '', reserved)\n",
    "    single = re.sub(pattern=single_letter_pattern, repl='', string=punctuation)\n",
    "    blank = re.sub(pattern=blank_spaces_pattern, repl=' ', string=single)\n",
    "    blank = blank.lower()\n",
    "    if stem_flag:\n",
    "        stemming = re.sub(r'/(ies|y|ed|ing|s)(\\s|\\b)/g', ' ', blank)\n",
    "\n",
    "    return blank.split() if not stem_flag else stemming.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processing(raw_tweet):\n",
    "    words = clean_tweets(raw_tweet)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    m_w = [w for w in words if not w in stops]\n",
    "    return (\" \".join(m_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "num_tweets = df_train[\"tweet\"].size\n",
    "clean_tweet = []\n",
    "for i in range(0,num_tweets):\n",
    "    clean_tweet.append(tweet_processing(df_train[\"tweet\"][i]))\n",
    "df_train[\"tweet\"] = clean_tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tokenized_sents'] = df_train.apply(lambda column: word_tokenize(column['tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, max_len=40):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['tokenized_sents'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['tokenized_sents'])\n",
    "    # Pad the sequence in order to have max length...\n",
    "    x = pad_sequences(sequences, maxlen=max_len)\n",
    "    # Make one hot encoding for labeling part. Should be in input [1 0]\n",
    "    y = to_categorical(data['emoji_class'], num_classes=7)\n",
    "    # splitting the Dataset into Train and Test set\n",
    "    x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2,random_state=0, shuffle=False)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = process_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pre = torch.from_numpy(y_train)\n",
    "y_test_pre = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "\n",
    "for i in range(len(y_train_pre)):\n",
    "    if 1 in y_train_pre[i,0]:\n",
    "        y_train.append(0)\n",
    "    if 1 in y_train_pre[i,1]:\n",
    "        y_train.append(1)\n",
    "    if 1 in y_train_pre[i,2]:\n",
    "        y_train.append(2)\n",
    "    if 1 in y_train_pre[i,3]:\n",
    "        y_train.append(3)\n",
    "    if 1 in y_train_pre[i,4]:\n",
    "        y_train.append(4)\n",
    "    if 1 in y_train_pre[i,5]:\n",
    "        y_train.append(5)\n",
    "    if 1 in y_train_pre[i,6]:\n",
    "        y_train.append(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "\n",
    "for i in range(len(y_test_pre)):\n",
    "    if 1 in y_test_pre[i,0]:\n",
    "        y_test.append(0)\n",
    "    if 1 in y_test_pre[i,1]:\n",
    "        y_test.append(1)\n",
    "    if 1 in y_test_pre[i,2]:\n",
    "        y_test.append(2)\n",
    "    if 1 in y_test_pre[i,3]:\n",
    "        y_test.append(3)\n",
    "    if 1 in y_test_pre[i,4]:\n",
    "        y_test.append(4)\n",
    "    if 1 in y_test_pre[i,5]:\n",
    "        y_test.append(5)\n",
    "    if 1 in y_test_pre[i,6]:\n",
    "        y_test.append(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "y_train_bow = y_train.squeeze()\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test_bow  = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bow = pd.DataFrame(x_train)\n",
    "x_test_bow = pd.DataFrame(x_test)\n",
    "# y_train_bow = pd.Series([y_train])\n",
    "# y_test_bow = pd.Series([y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# major_class_0,major_class_1, major_class_2, major_class_3, major_class_4, major_class_5, major_class_6 =bow_train.label.value_counts()\n",
    "# df_major0=bow_train[bow_train['label']==0]\n",
    "# df_major1=bow_train[bow_train['label']==1]\n",
    "# df_major2=bow_train[bow_train['label']==2]\n",
    "# df_major3=bow_train[bow_train['label']==3]\n",
    "# df_major4=bow_train[bow_train['label']==4]\n",
    "# df_major5=bow_train[bow_train['label']==5]\n",
    "# df_major6=bow_train[bow_train['label']==6]\n",
    "\n",
    "# df_minor1_upsampled = resample(df_major1, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor2_upsampled = resample(df_major2, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor3_upsampled = resample(df_major3, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor4_upsampled = resample(df_major4, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor5_upsampled = resample(df_major5, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor6_upsampled = resample(df_major6, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_bow_upsampled = pd.concat([df_major0, df_minor1_upsampled,df_minor2_upsampled, df_minor3_upsampled, df_minor4_upsampled, df_minor5_upsampled, df_minor6_upsampled])\n",
    "# print('shape',df_bow_upsampled.shape)\n",
    "# sns.countplot(df_bow_upsampled.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# major_class_0,major_class_1, major_class_2, major_class_3, major_class_4, major_class_5, major_class_6=tfidf_train.label.value_counts()\n",
    "# df_major0=tfidf_train[tfidf_train['label']==0]\n",
    "# df_major1=tfidf_train[tfidf_train['label']==1]\n",
    "# df_major2=tfidf_train[tfidf_train['label']==2]\n",
    "# df_major3=tfidf_train[tfidf_train['label']==3]\n",
    "# df_major4=tfidf_train[tfidf_train['label']==4]\n",
    "# df_major5=tfidf_train[tfidf_train['label']==5]\n",
    "# df_major6=tfidf_train[tfidf_train['label']==6]\n",
    "\n",
    "# df_minor1_upsampled = resample(df_major1, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor2_upsampled = resample(df_major2, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor3_upsampled = resample(df_major3, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor4_upsampled = resample(df_major4, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor5_upsampled = resample(df_major5, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_minor6_upsampled = resample(df_major6, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=major_class_0)\n",
    "# df_tfidf_upsampled  = pd.concat([df_major0, df_minor1_upsampled,df_minor2_upsampled, df_minor3_upsampled, df_minor4_upsampled, df_minor5_upsampled, df_minor6_upsampled])\n",
    "# print('shape',df_tfidf_upsampled.shape)\n",
    "# sns.countplot(df_tfidf_upsampled.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics of Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_(y_proba, y_pred):\n",
    "  y_true = y_proba[:,1] >= 0.3\n",
    "  y_true = y_true.astype(np.int) \n",
    "  f1 = f1_score(y_true, y_pred, average='micro') #  average setting, one of [None, 'micro', 'macro', 'weighted']\n",
    "  return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recall(y_true,y_pred):\n",
    "#   #y_true = y_true[:,1] >= 0.3\n",
    "#   y_true = y_true.astype(np.int) \n",
    "#   recall = recall_score(y_true, y_pred, average='macro') #  average setting, one of [None, 'micro', 'macro', 'weighted']\n",
    "#   return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true,y_pred):\n",
    "  #y_true = y_true[:,1] >= 0.3\n",
    "  y_true = y_true.astype(np.int) \n",
    "  recall_result = recall_score(y_true, y_pred, average='macro') #  average setting, one of [None, 'micro', 'macro', 'weighted']\n",
    "  return recall_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train_bow), x_train_bow.shape)\n",
    "#print(y_train_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Bow\n",
    "k=[3,5,7,11]\n",
    "accuracy=[]\n",
    "\n",
    "for i in tqdm(k):\n",
    "  model=KNeighborsClassifier(n_neighbors=i)\n",
    "  #model = MLPClassifier(random_state=1, max_iter=300)\n",
    "  model.fit(x_train_bow,y_train_bow)\n",
    "  y_pred=model.predict(x_test_bow)\n",
    "  acc=accuracy_score(y_pred,y_test_bow)\n",
    "  print('for k=',i,'Accuracy Score',acc)\n",
    "  accuracy.append(acc)\n",
    "  y_proba=model.predict_proba(x_test_bow)\n",
    "  f1_scor=f1_score_(y_proba,y_test_bow)\n",
    "  print('for k=',i,'f1 score ',f1_scor)\n",
    "  recall_result = recall(y_pred,y_test_bow)\n",
    "  print('for k=',i,'Accuracy Score',recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, X_train, train_y, max_epoch):\n",
    "\n",
    "  train_loss = []\n",
    "  train_acc = []\n",
    "  y_target =[]\n",
    "\n",
    "  # Divide the learning rate by 2 at each epoch, as suggested in paper\n",
    "  scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)     # Decays the learning rate of each parameter group by gamma every step_size epochs.                                                                            \n",
    "  epoch = 0                                                                         # Epochs done so far\n",
    "  stop = False   \n",
    "  correct=0\n",
    "  total=0\n",
    "\n",
    "  loss_func =  nn.MSELoss()\n",
    "\n",
    "\n",
    "  while epoch < max_epoch and not stop:                   \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    optimizer.zero_grad()                                                          # Sets the gradients of all optimized torch.Tensors to zero\n",
    "\n",
    "    yhat = model(X_train.float())                     # input x and predict based on \n",
    "    yhat = yhat.squeeze()\n",
    "    loss = loss_func(yhat.to(torch.float32), train_y.to(torch.float32))\n",
    "    loss.backward()                           # backward pass\n",
    "    optimizer.step()                          # gradient descent; Performs a single optimization step (parameter update)                                                                               \n",
    "      \n",
    "    #Print the statistic\n",
    "    running_loss += loss                                                \n",
    "    #running_acc += loss\n",
    "    epoch_loss = running_loss\n",
    "    epoch_acc = running_acc\n",
    "    print('Epoch {:d} -- Loss: {:.4f} '.format(epoch+1,epoch_loss))\n",
    "    #print('Epoch {:d} -- Loss: {:.4f} Acc: {:.4f}'.format(epoch+1,epoch_loss, epoch_acc))\n",
    "    epoch += 1\n",
    "    scheduler.step()                                                                 # This change the learning rate at each epoch, otherwise the LR would stays at the initial value                                             \n",
    "    train_loss.append(loss)\n",
    "    #train_acc= np.append(train_acc,epoch_acc)\n",
    "    #y_target.append(target_inds)\n",
    "  return train_loss, train_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tfidf\n",
    "k=[3,5,11]\n",
    "accuracy_tfidf=[]\n",
    "for i in k:\n",
    "  model=KNeighborsClassifier(n_neighbors=i)\n",
    "  model.fit(x_train_tfidf,y_train_tfidf)\n",
    "  y_pred=model.predict(x_test_tfidf)\n",
    "  acc=accuracy_score(y_pred,y_test_tfidf)\n",
    "  print('for k=',i,'Accuracy Score',acc)\n",
    "  accuracy_tfidf.append(acc)\n",
    "  y_proba=model.predict_proba(x_test_tfidf)\n",
    "  f1_scor=f1_score_(y_proba,y_test_tfidf)\n",
    "  print('for k=',i,'f1 score ',f1_scor)\n",
    "  \n",
    "  print('for k=',i,'Recall',recall)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0eb5d0a65b500759bcde1c4c1ad0551eaece71d5bef76353acf57400c52edb49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
